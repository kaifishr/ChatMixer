tag: ""
random_seed: 44

##############
# Dataloader #
##############
# shakespeare, lexicap
dataloader:
  dataset: "lexicap"
  num_workers: 2

#########
# Model #
#########
model:
  sequence_length: 256
  num_dims: 128  # dim, embedding_dims
  token_dims: 1024  # dims_hidden_token
  channel_dims: 1024  # token_dims = channel_dims = 256, dims_hidden_channel
  num_blocks: 32
  # Available encoding schemes: ones, zeros, random_normal
  position_embedding:
    is_activated: true
    is_trainable: true
    encoding: "zeros"
  mixer_block:
    dropout_prob: 0.05
    use_bias: True
  mlp_block:
    dropout_prob: 0.05
    use_bias: True
  classifier:
    use_bias: True

###########
# Trainer #
###########
trainer:
  device: "gpu"   # gpu, cpu
  start_update_step: 0
  num_update_steps: 100000
  batch_size: 64
  weight_decay: 0
  initial_learning_rate: 1.0e-6
  max_learning_rate: 0.001
  grad_norm_clip: 1.0  # TODO: Use version below.
  gradient_clipping:
    is_activated: true
    max_norm: 1.0

load_model:
  is_activated: false 
  model_name: "lexicap"
  model_path: -1

########
# Data #
########
data:
  n_classes: Null
  input_shape: Null

###########
# Summary #
###########
summary:
  save_train_stats:
    every_n_updates: 100
  save_test_stats:
    every_n_updates: -1
  save_model:
    every_n_updates: 5000
  add_patch_embeddings:
    every_n_updates: -1 
  add_position_embeddings:
    every_n_updates: 1000
  add_token_embeddings:
    every_n_updates: 1000
  add_linear_weights:
    every_n_updates: 1000
  add_params_hist:
    every_n_updates: -1
  add_graph: false
  add_sample_batch: false
  add_hparams: false

###############
# Directories #
###############
dirs:  # TODO: dirs -> dir
  data: "data"
  runs: "runs"
  weights: "weights"